# -*- coding: utf-8 -*-
"""Code for graduation work_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bf_7nNQxVqDVZNFE6bNPOnEd9EXTvoQf

#Подгрузка библиотек

В ячейку ниже будут подгружаться библиотеки по мере надобности
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from sklearn.model_selection import cross_val_score

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

print(tf.__version__)

# %matplotlib inline

"""#Подключение google disc

Необходимые файлы хранятся на нем
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Загрузка файлов в рабочую среду

Загрузка файла 1 (X_bp.xlsl)
"""

df1 = pd.read_excel("/content/drive/MyDrive/ВКР/X_bp.xlsx",dtype=np.float64)
pd.set_option("display.precision", 3)

"""Прверка, подгрузился ли файл 1"""

df1.shape

df1.head()

df1.info()

# перевод в csv

df1.to_csv('X_bp.xlsl.csv', encoding='utf-8')

df1.shape, df1.info()

df1.head()

"""Загрузка файла 2 (X_nup.xlsl)"""

df2 = pd.read_excel("/content/drive/MyDrive/ВКР/X_nup.xlsx")

"""Проверка, подгрузился ли файл 2"""

df2.head()

df2.shape

df2.info()

# перевод в csv

df2.to_csv('X_nup.xlsx.csv', encoding='utf-8')

df2.shape, df2.info

df2.head()

"""#Объединение двух файлов методом INNER


"""

DF = df1.merge(df2, how = "inner")

"""Проверка объединения двух фалов"""

DF.head()

"""Делаю копию DF"""

DF_copy = DF.copy()

DF_copy.head()

"""Промежуточный итог: загрузка и объединение фалов прошло успешно.

#Разведочный анализ данных
— анализ основных свойств данных, нахождение в них общих закономерностей, распределений и аномалий, построение начальных моделей, зачастую с использованием инструментов визуализации.

Необходимо нарисовать гистограммы распределения каждой из переменной, диаграммы ящика с усами, попарные графики рассеяния точек. Необходимо также для каждой колонке получить среднее, медианное значение, провести анализ и исключение выбросов, проверить наличие пропусков.

Сначала посмотрим общую информацию
"""

DF.info()

"""Всего 15 колонок, 1023 записи, тип записей float64;
Названия колонок: 


 0   Unnamed: 0_x                             
 1   Соотношение матрица-наполнитель          
 2   Плотность, кг/м3                      
 3   модуль упругости, ГПа                 
 4   Количество отвердителя, м.%           
 5   Содержание эпоксидных групп,%_2       
 6   Температура вспышки, С_2              
 7   Поверхностная плотность, г/м2         
 8   Модуль упругости при растяжении, ГПа  
 9   Прочность при растяжении, МПа         
 10  Потребление смолы, г/м2              
 11  Unnamed: 0_y                         
 12  Угол нашивки, град                    
 13  Шаг нашивки                           
 14  Плотность нашивки                     




"""

# можно сделать это с помощью кода
DF.columns.tolist()

"""Выведем статистику"""

DF.shape

DF.describe()

"""Для удобства прочтения выведем статистику с помощью транспонирования"""

DF.describe().T

""" Из анализа выше мы видим, что признак "Угол нашивки, град" имеет два значения: 0 и 90, поэтому квартили и средние значения для него не имеют смысла.
 Проверим предположение.


"""

DF["Угол нашивки, град"].value_counts()

"""Предположение подтвердилось, данный признак бинарен. Имеются 520 значений 0 градусов и 503 значений 90 градусов, что при данной выборке можно с натяжкой назвать примерно одинаковым.

Смотрим пропущенные значения
"""

DF.isna().sum()

"""Пропущенных значений нет

Посомтрим дисперсию
"""

DF.var()

"""Отсортируем значения признаков"""

mean_DF = DF.mean().sort_values()
mean_DF

"""Построим матрицу корреляции между всеми исследуемыми показателями

---


"""

DF_corr = DF.corr()
DF_corr

"""Посмотрим  на визуализации тепловой карты из Seaborn"""

plt.figure(figsize=(20,10))

DF_heatmap = sns.heatmap(DF.corr(), annot = True, cmap="Greens")

DF_heatmap.set_title(" Тепловая карта корреляции между всеми исследуемыми показателями", fontsize = 16, pad = 20)

plt.show(DF_heatmap)

"""Из тепловой гистограммы можно сделать вывод, что в предоставленных данных не видно явных зависимостей. Можно сделать предположения, что стоит обратить внимание на пары "Плотность нашивки" и "Угол нашивки", "Плотность нашивки" и "модуль упругости", "Плотность нашивки" и "Плотность", "Количество отвердителя" и "Поверхностная плотность", "Плотность" и "Поверхностная плотность", "Потребление смолы" и "Температура вспышки", "Поверхностная плотность" и "Температура вспышки".

Наиболее сильную корреляцию имеют "Количество отвердителя" и "Температура вспышки".

"Прочность при растяжении" и "Модуль упругости", "Соотношение матрица-наполнитель"  и "Модуль упругости" имеют не явно выраженную зависимость. Из этого можно сделать предположение, что последующее исследование на предоставленных данных может не дать существенных результатов.

#Сделаем попарные попарные графики рассеяния точек
"""

sns.pairplot(DF);

"""Сделаем попарные гистограммы выделенных выше пар
"Плотность нашивки" и "Угол нашивки", "Плотность нашивки" и "модуль упругости", "Плотность нашивки" и "Плотность", "Количество отвердителя" и "Поверхностная плотность", "Плотность" и "Поверхностная плотность", "Потребление смолы" и "Температура вспышки", "Поверхностная плотность" и "Температура вспышки".

Наиболее сильную корреляцию имеют "Количество отвердителя" и "Температура вспышки".

"Прочность при растяжении" и "Модуль упругости", "Соотношение матрица-наполнитель" и "Модуль упругости" имеют не явно выраженную зависимость.
"""

sns.pairplot(DF[['Плотность нашивки', 'Угол нашивки, град']])
sns.pairplot(DF[['Плотность нашивки', 'модуль упругости, ГПа']])
sns.pairplot(DF[['Плотность нашивки', 'модуль упругости, ГПа']])
sns.pairplot(DF[['Плотность нашивки', 'Плотность, кг/м3']])
sns.pairplot(DF[['Количество отвердителя, м.%', 'Поверхностная плотность, г/м2']])
sns.pairplot(DF[['Потребление смолы, г/м2', 'Поверхностная плотность, г/м2']])
sns.pairplot(DF[['Потребление смолы, г/м2', 'Температура вспышки, С_2']])
sns.pairplot(DF[['Прочность при растяжении, МПа', 'модуль упругости, ГПа']])
sns.pairplot(DF[['Соотношение матрица-наполнитель', 'модуль упругости, ГПа']]);

"""Построим гистограмму для каждого признака"""

DF.hist(["Соотношение матрица-наполнитель"]);
DF.hist(["Плотность, кг/м3"])
DF.hist(["модуль упругости, ГПа"])
DF.hist(["Количество отвердителя, м.%"])
DF.hist(["Содержание эпоксидных групп,%_2"])
DF.hist(["Температура вспышки, С_2"])
DF.hist(["Поверхностная плотность, г/м2"])
DF.hist(["Модуль упругости при растяжении, ГПа"])
DF.hist(["Потребление смолы, г/м2"])
DF.hist(["Угол нашивки, град"])
DF.hist(["Шаг нашивки"])
DF.hist(["Плотность нашивки"]);

"""Построим попарный график корреляции"""

grid = sns.PairGrid(DF)

grid = grid.map(plt.scatter);

"""Построим ящики с усами для всех колонок"""

plt.figure(figsize=(80,50))
sns.boxplot(data=DF,palette='Set2');

"""Данная гистограмма оказалась не слишком информативной, поскольку данные не нормализованы. Размеры разные, и тяжело увидеть, где есть выбросы. Поэтому построим ящики с усами для каждого признака отдельно."""

for col in DF.columns:
  plt.figure(figsize=(8,16))
  plt.title(str(col))
  sns.boxplot(data=DF[col], palette = "Set2");
  plt.show()

"""По ящикам с усами видно, что выбросы присутствуют у всех признаков.

#Исключение выбросов

Выбираем столбцы с выбросами из визуализации колонок ящиков с усами выше и удаляем выбросы
"""

#заменяем выбросы на NaN

for x in DF.columns:
    q75,q25 = np.percentile(DF.loc[:,x],[75,25])
    intr_qr = q75-q25

    max = q75+(1.5*intr_qr)
    min = q25-(1.5*intr_qr)

    DF.loc[DF[x] < min,x] = np.nan
    DF.loc[DF[x] > max,x] = np.nan

# выбросы по каждому столбцу
DF.isnull().sum()

# удаляем выбросы

DF= DF.dropna(axis=0)

# проверяем

plt.figure(figsize=(80,50))
sns.boxplot(data=DF,palette='Set2');

DF.head()

# еще раз построим гистограммы
for col in DF.columns:
  plt.title(str(col))
  sns.histplot(data=DF[col])
  plt.show()

"""Убраны выбросы

#Проведем нормализацию
"""

DF.head()

minmax_scaler= MinMaxScaler()

DF_norm = minmax_scaler.fit_transform(DF)

# Преобразуем обратно в dataframe

DF_norm = pd.DataFrame(data=DF_norm, columns = ["Unnamed: 0", "Соотношение матрица-наполнитель", "Плотность, кг/м3", "модуль упругости, ГПа", "Количество отвердителя, м.%", "Содержание эпоксидных групп,%_2", "Температура вспышки, С_2", "Поверхностная плотность, г/м2", "Модуль упругости при растяжении, ГПа", "Прочность при растяжении, МПа", "Потребление смолы, г/м2", "Угол нашивки, град", "Шаг нашивки", "Плотность нашивки"] )

DF_norm.head()

DF_norm.describe()

"""Нормализовали данные.
Теперь датафрейм обозначается как 

#DF_norm

#Обучение моделей для прогноза модуля упругости при растяжении и прочности при растяжении

Для этого нужно разделить данные  на тестовую(30%) и тренировочную выборки

#Прогноз прочности при растяжении

Делим датасет на тестовую (30%) и тренировочную выборки. За X берем датафрейм, за y - колонку Прочность при растяжении, МПа.
"""

DF_norm.head()

"""Сделаем копию данных"""

DF_learning = DF_norm.copy()

DF_learning.head()

"""#Прочность при растяжении"""

X = DF_learning.drop(columns=[ 'Прочность при растяжении, МПа'])
y = DF_learning[['Прочность при растяжении, МПа']]

X.shape

y.shape

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

"""Обучить нескольких моделей для прогноза модуля упругости при растяжении и прочности при растяжении. При построении модели необходимо 30% данных оставить на тестирование модели, на остальных происходит обучение моделей. При построении моделей провести поиск гиперпараметров модели с помощью поиска по сетке с перекрестной проверкой, количество блоков равно 10

Линейная регрессия (прочность при растяжении)
"""

lr1 = LinearRegression()

#обучаем

lr1.fit(X_train, y_train)

# предсказание

y_pred_lr1 = lr1.predict(X_test)

# проверяем

r2_lr1 = lr1.score(X_test, y_test)
mse_lr1 = mean_squared_error (y_test,y_pred_lr1)
mae_lr1 = mean_absolute_error (y_test,y_pred_lr1)

print(r2_lr1, mse_lr1, mae_lr1)

"""Посмотрим график"""

plt.scatter(y_test, y_pred_lr1)
plt.xlabel("Значения из таблицы")
plt.ylabel("Значения предсказаний")
limits = [0,1]
plt.xlim(limits)
plt.ylim(limits);

# кросс валидация
CV = cross_val_score(lr1, X = X_train, y = y_train, cv = 10)
print(CV)
print(CV.mean())

"""Вывод: очень не очень.

Попробуем случайный лес,регрессор
"""

y.shape

X.shape

Rfr = RandomForestRegressor( max_depth = 7, random_state=0)

Rfr.fit(X_train, y_train.values.ravel())

y_predict_Rfr = Rfr.predict(X_test)

Rfr.score(X_train, y_train)

r2_score(y_test, y_predict_Rfr)

plt.scatter(y_test, y_predict_Rfr)
plt.xlabel("Значения из таблицы")
plt.ylabel("Значения предсказаний")
limits = [0,1]
plt.xlim(limits)
plt.ylim(limits);

"""Вывод: не подходит"""

# кросс валидация
CV_Rfr = cross_val_score(Rfr, X = X_train, y = y_train.values.ravel(), cv = 10)
print(CV_Rfr)
print(CV_Rfr.mean())

"""Решающие деревья"""

Dtr = DecisionTreeRegressor()

Dtr.fit(X_train, y_train)

y_predict_Dtr = Dtr.predict(X_test)

Dtr.score(X_train, y_train)

r2_Dtr = Dtr.score(X_test, y_test)
mse_Dtr = mean_squared_error (y_test,y_predict_Dtr)
mae_Dtr = mean_absolute_error (y_test,y_predict_Dtr)

print(r2_Dtr, mse_Dtr, mae_Dtr)

plt.scatter(y_test, y_predict_Dtr)
plt.xlabel("Значения из таблицы")
plt.ylabel("Значения предсказаний")
limits = [0,1]
plt.xlim(limits)
plt.ylim(limits);

CV_Dtr = cross_val_score(Dtr, X = X_train, y = y_train, cv = 10)
print(CV_Dtr)
print(CV_Dtr.mean())

"""И опять не то.

Итог: для Прочности при растяжении не найдено зависимых параметров, приличный прогноз сделать нельзя. Попробуем поискать для Модуля упругости при растяжении.

#Модуль упругости при растяжении
"""

X = DF_learning.drop(columns=[ 'Модуль упругости при растяжении, ГПа'])
y = DF_learning[['Модуль упругости при растяжении, ГПа']]

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

lr2 = LinearRegression()

#обучаем

lr2.fit(X_train, y_train)

# предсказание

y_pred_lr2 = lr2.predict(X_test)

# проверяем

r2_lr2 = lr2.score(X_test, y_test)
mse_lr2 = mean_squared_error (y_test,y_pred_lr2)
mae_lr2 = mean_absolute_error (y_test,y_pred_lr2)

print(r2_lr1, mse_lr2, mae_lr2)

plt.scatter(y_test, y_pred_lr2)
plt.xlabel("Значения из таблицы")
plt.ylabel("Значения предсказаний")
limits = [0,1]
plt.xlim(limits)
plt.ylim(limits);

CV_lr2 = cross_val_score(lr2, X = X_train, y = y_train, cv = 10)
print(CV_lr2)
print(CV_lr2.mean())

"""Случайный лес, регрессор"""

Rfr1 = RandomForestRegressor( max_depth = 1, random_state=0)

Rfr1.fit(X_train, y_train.values.ravel())

y_predict_Rfr1 = Rfr1.predict(X_test)

Rfr1.score(X_train, y_train)

Rfr1.score(X_train, y_train)

r2_score(y_test, y_predict_Rfr1)

plt.scatter(y_test, y_predict_Rfr1)
plt.xlabel("Значения из таблицы")
plt.ylabel("Значения предсказаний")
limits = [0,1]
plt.xlim(limits)
plt.ylim(limits);

CV_Rfr1 = cross_val_score(Rfr1, X = X_train, y = y_train.values.ravel(), cv = 10)
print(CV_Rfr1)
print(CV_Rfr1.mean())

"""Решающие деревья"""

Dtr1 = DecisionTreeRegressor()

Dtr1.fit(X_train, y_train)

y_predict_Dtr1 = Dtr1.predict(X_test)

Dtr1.score(X_train, y_train)

r2_Dtr1 = Dtr1.score(X_test, y_test)
mse_Dtr1 = mean_squared_error (y_test,y_predict_Dtr1)
mae_Dtr1 = mean_absolute_error (y_test,y_predict_Dtr1)

print(r2_Dtr1, mse_Dtr1, mae_Dtr1)

plt.scatter(y_test, y_predict_Dtr1)
plt.xlabel("Значения из таблицы")
plt.ylabel("Значения предсказаний")
limits = [0,1]
plt.xlim(limits)
plt.ylim(limits);

CV_Dtr1 = cross_val_score(Dtr1, X = X_train, y = y_train, cv = 10)
print(CV_Dtr1)
print(CV_Dtr1.mean())

"""Итог: для Модуля упругости при растяжении не найдено зависимых параметров, приличный прогноз сделать нельзя.

##Написать нейронную сеть, которая будет рекомендовать соотношение матрица. 
Описывается выбранная архитектура нейронной сети и ее результаты.
"""

DF_norm.head()

DF_norm.shape

X_N = DF.drop(columns=['Соотношение матрица-наполнитель'])
y_N = DF_norm['Соотношение матрица-наполнитель']

X_N_train, X_N_test, y_N_train, y_N_test = train_test_split(X_N,y_N, test_size=0.3, random_state=42)

model_N_start = Sequential([
                      Dense(14,activation = "relu", input_shape=(13,)),
                      Dense(7, activation = "relu"),
                      Dense(1)                
                      ])

model_N_start.compile(optimizer = "adam", loss = "categorical_crossentropy")

history_start = model_N_start.fit(X_N_train, y_N_train, batch_size = 50, epochs = 18, verbose = 1,
                       validation_data=(X_N_test, y_N_test))

plt.plot(history_start.history['loss'],
         label = 'loss')

plt.plot(history_start.history['val_loss'],
         label = 'val_loss')

plt.xlabel('Эпоха обучения')
plt.ylabel('Ошибки')
plt.legend()
plt.show()

model_N_start.summary()

"""Итог: нейронная сеть не работает совсем

Пробуем другую модель
"""

model_N = Sequential([
                      Dense(14,activation = "relu", input_shape=(13,)),
                      Dense(7, activation = "relu"),
                      Dense(1)                
                      ])

model_N.compile(optimizer = "adam", loss = "mae")

history = model_N.fit(X_N_train, y_N_train, batch_size = 50, epochs = 50, verbose = 1,
                       validation_data=(X_N_test, y_N_test))

plt.plot(history.history['loss'],
         label = 'loss')

plt.plot(history.history['val_loss'],
         label = 'val_loss')

plt.xlabel('Эпоха обучения')
plt.ylabel('Ошибки')
plt.legend()
plt.show()

y_pred_N = model_N.predict(X_N_test)

plt.figure(figsize=(10,5))
sns.kdeplot(data=y_pred_N, label = 'y_pred_N', color='b')
sns.kdeplot(data=y_N_test, label = 'y_N_test', color='orange')
plt.xlabel("Значение")
plt.legend()  
plt.show()

model_N.summary()

"""Лучше, чем первая, но цель не достигается

Добавим линейную функцию активации в последнем слое
"""

model_N1 = Sequential([
                      Dense(14,activation = "relu", input_shape=(13,)),
                      Dense(7, activation = "relu"),
                      Dense(1, activation = "linear")                 
                      ])

model_N1.compile(optimizer = "adam", loss = "mae")

history_N1 = model_N1.fit(X_N_train, y_N_train, batch_size = 50, epochs = 20, verbose = 1,
                       validation_data=(X_N_test, y_N_test))

plt.plot(history_N1.history['loss'],
         label = 'loss')

plt.plot(history_N1.history['val_loss'],
         label = 'val_loss')

plt.xlabel('Эпоха обучения')
plt.ylabel('Error')
plt.legend()
plt.show()

y_pred_N1 = model_N1.predict(X_N_test)

plt.figure(figsize=(10,5))
sns.kdeplot(data=y_pred_N1, label = 'y_pred_N1', color='b')
sns.kdeplot(data=y_N_test, label = 'y_N_test', color='orange')
plt.xlabel("Значение")
plt.legend()  
plt.show()

model_N1.summary()

"""В итоге, обучение длится 3 эпохи, потом все.

Попробуем в качестве функции потерь mse и добавим слой
"""

model_N2 = Sequential([
                      Dense(14,activation = "relu", input_shape=(13,)),
                      Dense(7, activation = "relu"),
                      Dense(3, activation = "relu"),
                      Dense(1, activation="softmax")                 
                      ])

model_N2.compile(optimizer = "adam", loss = "mse", metrics = "mae")

history2 = model_N.fit(X_N_train, y_N_train, batch_size = 50, epochs = 20, verbose = 1,
                       validation_data=(X_N_test, y_N_test))

plt.plot(history2.history['loss'],
         label = 'loss')

plt.plot(history2.history['val_loss'],
         label = 'val_loss')

plt.xlabel('Эпоха обучения')
plt.ylabel('Ошибки')
plt.legend()
plt.show()

y_pred_N2 = model_N2.predict(X_N_test)

plt.figure(figsize=(10,5))
sns.kdeplot(data=y_pred_N2, label = 'y_pred_N2', color='b')
sns.kdeplot(data=y_N_test, label = 'y_N_test', color='orange')
plt.xlabel("Значение")
plt.legend()  
plt.show();

model_N2.summary()

"""Даже при увеличении количества эпох, результата нет

Итог: более-менее удовлитворительное решение задачи не найдено

Выбирая из плохих и очень плохих моделей, выбрала model_N
"""

3434#Ввод параметров и получение списка с параметрами

print("Введите параметры для прогноза соотношения 'Матрица-наполнитель'")
print("Unnamed: 0, Введите 1")
p1 = np.float64(input())
print("Плотность, кг/м3")
p2 = np.float64(input())
print("Модуль упругости, ГПа")
p3 = np.float64(input())
print("Количество отвердителя, м.%")
p4 = np.float64(input())
print("Содержание эпоксидных групп,%_2")
p5 = np.float64(input())
print("Температура вспышки, С_2")
p6 = np.float64(input())
print("Поверхностная плотность, г/м2")
p7 = np.float64(input())
print("Модуль упругости при растяжении, ГПа")
p8 = np.float64(input())
print("Прочность при растяжении, МПа")
p9 = np.float64(input())
print("Потребление смолы, г/м2")
p10 = np.float64(input())
print("Угол нашивки, град")
p11 = np.float64(input())
print("Шаг нашивки")
p12 = np.float64(input())
print("Плотность нашивки")
p13 = np.float64(input())

args = np.array([[p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12, p13]])

# кладу в предикт аргументы, т.е. параметры, т.е. колонки таблицы без соотношение матрица-наполнитель
predict_result = model_N.predict(args)

#возвращаем исходный масштаб значения, т.е. используем датасет до нормализации
predict_result2 = predict_result * np.max(DF['Соотношение матрица-наполнитель']) + np.min(DF['Соотношение матрица-наполнитель'])

print("Рекомендуемое соотношение 'Матрица-наполнитель' =", predict_result2[0,0])

"""В принципе, работает."""





